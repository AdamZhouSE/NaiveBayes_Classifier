{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T03:00:57.440226Z",
     "start_time": "2024-11-28T03:00:55.401361Z"
    }
   },
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), 'dataset-news')\n",
    "content_list = []\n",
    "label_list = []\n",
    "for file in os.listdir(dataset_path):\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        # get the prefix of the file name as the target\n",
    "        prefix = re.match(r\"^[^\\d]+\", file)\n",
    "        label_list.append(prefix.group())\n",
    "        # read file content\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            content_list.append(content)\n",
    "\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "X = count_vect.fit_transform(content_list).toarray()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(np.array(label_list))\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "gnb_accuracies = []\n",
    "mnb_accuracies = []\n",
    "# use 10-fold cross validation\n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    gnb.fit(X_train, y_train)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    y_pred_gnb = gnb.predict(X_test)\n",
    "    y_pred_mnb = mnb.predict(X_test)\n",
    "    gnb_accuracies.append(accuracy_score(y_test, y_pred_gnb))\n",
    "    mnb_accuracies.append(accuracy_score(y_test, y_pred_mnb))\n",
    "\n",
    "print(f'GaussianNB accuracy: {np.mean(gnb_accuracies):.2f}')\n",
    "print(f'MultinomialNB accuracy: {np.mean(mnb_accuracies):.2f}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB accuracy: 0.72\n",
      "MultinomialNB accuracy: 0.79\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**(a) Answer**\n",
    "\n",
    "GaussianNB accuracy: 0.72\n",
    "\n",
    "MultinomialNB accuracy: 0.79\n",
    "\n",
    "The multinomial Naive Bayes classifier performs better than the Gaussian Naive Bayes classifier. The multinomial Naive Bayes classifier is more suitable for text classification tasks, as it models the frequency of words in the document. The Gaussian Naive Bayes classifier assumes that the features are normally distributed, which may not be the case for text data. Therefore, the multinomial Naive Bayes classifier is more appropriate for text classification tasks."
   ],
   "id": "b9887d8550c0ee28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T02:40:22.368288Z",
     "start_time": "2024-11-28T02:40:22.365532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# random classifier\n",
    "true_labels = y\n",
    "random_labels = np.random.randint(0, 8, 800)\n",
    "accuracy = accuracy_score(true_labels, random_labels)\n",
    "print(f'Random Classifier Accuracy: {accuracy:.2f}')"
   ],
   "id": "ba631887f1a72317",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Classifier Accuracy: 0.13\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**(b) Answer**\n",
    "\n",
    "Random Classifier Accuracy: 0.13\n",
    "\n",
    "The performance of the random classifier is very poor, which is expected. The random classifier assigns a random label to each instance. As there are 8 labels in total, the probability of assigning the correct label to an instance should be around 1/8 = 0.125."
   ],
   "id": "b9a2b47806004bd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T03:01:29.127060Z",
     "start_time": "2024-11-28T03:01:28.003551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Do not remove the stop words\n",
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(content_list).toarray()\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    mnb.fit(X_train, y_train)\n",
    "    y_pred_mnb = mnb.predict(X_test)\n",
    "    mnb_accuracies.append(accuracy_score(y_test, y_pred_mnb))\n",
    "\n",
    "print(f'MultinomialNB accuracy: {np.mean(mnb_accuracies):.2f}')"
   ],
   "id": "bd29e6f39dceeeca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy: 0.76\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**(c) Answer**\n",
    "\n",
    "MultinomialNB accuracy with stopwords: 0.76\n",
    "\n",
    "The accuracy of the Multinomial Naive Bayes classifier decreases when stop words are not removed. Stop words are common words that do not carry much information about the content of the text, such as \"the\", \"is\", \"and\", etc. By removing stop words, we can reduce the dimensionality of the feature space and focus on more meaningful words that can help in classification. When stop words are not removed, the classifier may be influenced by these common words, leading to a decrease in accuracy."
   ],
   "id": "d2ecafc44f19feaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T03:21:52.899191Z",
     "start_time": "2024-11-28T03:21:52.683673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_task_documents(topics):\n",
    "    task_content_list = []\n",
    "    task_label_list = []\n",
    "    for i in range(0, len(content_list)):\n",
    "        if label_list[i] in topics:\n",
    "            task_content_list.append(content_list[i])\n",
    "            task_label_list.append(label_list[i])\n",
    "    return task_content_list, task_label_list\n",
    "\n",
    "\n",
    "def evaluate_task(topics):\n",
    "    task_content_list, task_label_list = get_task_documents(topics)\n",
    "    count_vect = CountVectorizer(stop_words='english')\n",
    "    X = count_vect.fit_transform(task_content_list).toarray()\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(np.array(task_label_list))\n",
    "    for train, test in kf.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "        mnb.fit(X_train, y_train)\n",
    "        y_pred_mnb = mnb.predict(X_test)\n",
    "        mnb_accuracies.append(accuracy_score(y_test, y_pred_mnb))\n",
    "    return np.mean(mnb_accuracies)\n",
    "\n",
    "task1_topics = [\"pol-guns\", \"hockey\", \"mac-hw\"]\n",
    "task2_topics = [\"mac-hw\", \"ibm-hw\", \"electronics\"]\n",
    "print(f'Task 1 Accuracy: {evaluate_task(task1_topics):.2f}')\n",
    "print(f'Task 2 Accuracy: {evaluate_task(task2_topics):.2f}')"
   ],
   "id": "d91bcdb566ab77fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Accuracy: 0.82\n",
      "Task 2 Accuracy: 0.79\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**(d) Answer**\n",
    "\n",
    "Task 1 Accuracy: 0.82\n",
    "\n",
    "Task 2 Accuracy: 0.79\n",
    "\n",
    "The accuracy of the Multinomial Naive Bayes classifier is higher for Task 1 compared to Task 2. This indicates that the classifier performs better on documents related to the topics \"use of guns\", \"hockey\", and \"Mac hardware\" compared to the topics \"Mac hardware\", \"IBM hardware\", and \"electronics\".\n",
    "\n",
    "Task 1 achieves higher accuracy due to clear topic distinctions. The vocabulary overlap is minimal, so the classifier can distinguish classes easily. However, task 2 has lower accuracy because of overlapping vocabulary and semantics among the topics. The topics are related. Significant vocabulary overlap (e.g., terms like “hardware”, “device”) may confuse the classifier."
   ],
   "id": "13691a0385d0d9c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
